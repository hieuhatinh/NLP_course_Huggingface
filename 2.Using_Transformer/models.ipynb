{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\FPTSHOP\\miniconda3\\envs\\aio2024-exercise\\Lib\\site-packages\\transformers\\utils\\generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "c:\\Users\\FPTSHOP\\miniconda3\\envs\\aio2024-exercise\\Lib\\site-packages\\transformers\\utils\\generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "c:\\Users\\FPTSHOP\\miniconda3\\envs\\aio2024-exercise\\Lib\\site-packages\\transformers\\utils\\generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "# Building the config\n",
    "config = BertConfig()\n",
    "\n",
    "# Building the model from the config\n",
    "model = BertModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.35.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Different loading methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a model from the default configuration initializes it with random values\n",
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "config = BertConfig()\n",
    "model = BertModel(config)\n",
    "\n",
    "# Model is randomly initialized!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading a Transformer model that is already trained\n",
    "from transformers import BertModel\n",
    "\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Saving methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"directory_on_my_computer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using a Transformer model for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-3.6115e-02,  1.3875e-01, -5.9633e-01,  ...,  1.8508e+00,\n",
       "           1.3610e-01, -1.1402e+00],\n",
       "         [ 2.0590e+00,  6.5385e-01,  2.9579e-01,  ...,  1.9615e+00,\n",
       "           1.2386e+00, -1.3576e+00],\n",
       "         [ 9.7887e-01, -7.1712e-02,  4.9661e-02,  ..., -2.9707e-01,\n",
       "          -7.4783e-01, -4.6061e-01],\n",
       "         [ 1.6249e+00, -9.9143e-01, -5.1973e-01,  ...,  7.4564e-01,\n",
       "           1.8475e-01, -1.5595e+00]],\n",
       "\n",
       "        [[-4.5805e-01,  7.9071e-01, -7.7409e-01,  ...,  5.8365e-01,\n",
       "          -1.1566e+00, -8.1661e-01],\n",
       "         [ 1.6365e+00,  5.0146e-01,  1.1510e+00,  ..., -6.6507e-01,\n",
       "           1.2256e+00, -1.4406e+00],\n",
       "         [ 2.3916e+00,  8.2469e-01,  4.4288e-01,  ..., -8.8774e-01,\n",
       "          -1.6451e+00,  1.2157e-01],\n",
       "         [ 2.5825e+00, -3.5125e-02,  4.9824e-04,  ...,  7.7809e-01,\n",
       "           2.5670e-01, -1.7848e+00]],\n",
       "\n",
       "        [[ 5.0870e-01,  2.2588e-01,  2.0518e-01,  ...,  1.0843e+00,\n",
       "          -1.5951e-01, -1.3951e+00],\n",
       "         [ 9.2512e-01,  2.2285e+00,  2.2037e+00,  ..., -2.9799e-01,\n",
       "           7.4615e-01, -1.2119e+00],\n",
       "         [ 1.3641e+00,  4.0802e-01,  5.1457e-01,  ..., -5.7597e-01,\n",
       "          -9.2943e-01,  5.8222e-01],\n",
       "         [ 4.9344e-02,  4.8527e-02,  7.8518e-01,  ...,  1.2176e+00,\n",
       "          -4.9362e-01, -1.4878e+00]]], grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[ 0.2895, -0.2397, -0.3936,  ..., -0.4264,  0.5613, -0.6064],\n",
       "        [ 0.2517, -0.5969, -0.3792,  ..., -0.8609, -0.1699, -0.1458],\n",
       "        [-0.1911,  0.0219, -0.5822,  ..., -0.5518, -0.2503,  0.0196]],\n",
       "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "sequences = [\"Hello!\", \"Cool.\", \"Nice!\"]\n",
    "\n",
    "encoded_sequences = [\n",
    "    [101, 7592, 999, 102],\n",
    "    [101, 4658, 1012, 102],\n",
    "    [101, 3835, 999, 102],\n",
    "]\n",
    "model_inputs = torch.tensor(encoded_sequences)\n",
    "\n",
    "output = model(model_inputs)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 768])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.last_hidden_state.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aio2024-exercise",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
